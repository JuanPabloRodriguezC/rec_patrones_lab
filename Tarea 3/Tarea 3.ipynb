{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from matplotlib import pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerceptronMulticapa:\n",
    "    def __init__(self, capas, activaciones, alpha=0.1):\n",
    "        self.capas = capas\n",
    "        self.alpha = alpha\n",
    "        self.activaciones = activaciones\n",
    "        self.bias = []\n",
    "        self.pesos = []\n",
    "        for i in range(0, len(capas) - 1):\n",
    "            # Inicializar los pesos y bias de cada capa (He para ReLU, Xavier para otras)\n",
    "            if activaciones[i] == \"relu\":\n",
    "                peso = np.random.randn(capas[i], capas[i+1]) * np.sqrt(2.0 / capas[i])\n",
    "            else:\n",
    "                peso = np.random.randn(capas[i], capas[i+1]) * np.sqrt(1.0 / capas[i])\n",
    "            self.pesos.append(peso)\n",
    "            bias = np.zeros(capas[i+1])\n",
    "            self.bias.append(bias)\n",
    "\n",
    "    def activacion(self, x, activacion):\n",
    "        match activacion:\n",
    "            case \"tanh\":\n",
    "                return np.tanh(x)\n",
    "            case \"sigmoid\":\n",
    "                return 1 / (1 + np.exp(-x))\n",
    "            case \"relu\":\n",
    "                return np.maximum(0, x)\n",
    "            case \"lineal\":\n",
    "                return x\n",
    "            case \"softmax\":\n",
    "                # suponemos entrada 1D por muestra\n",
    "                e_x = np.exp(x - np.max(x))\n",
    "                return e_x / np.sum(e_x)\n",
    "\n",
    "    def activacion_derivada(self, x, activacion):\n",
    "        # Aquí 'x' es la activación (no el pre-activation z) — coincide con tu uso actual.\n",
    "        match activacion:\n",
    "            case \"tanh\":\n",
    "                return 1 - x ** 2\n",
    "            case \"sigmoid\":\n",
    "                return x * (1 - x)\n",
    "            case \"relu\":\n",
    "                return (x > 0).astype(float)\n",
    "            case \"lineal\":\n",
    "                return np.ones_like(x)\n",
    "            case \"softmax\":\n",
    "                # Si usas softmax + cross-entropy, la derivada se maneja en el delta (y_pred - y),\n",
    "                # así que no la usamos aquí; devolvemos 1s para no romper formas si se usa.\n",
    "                return np.ones_like(x)\n",
    "\n",
    "    def feedforward(self, X):\n",
    "        # Calcular la salida de cada capa (lista de activaciones por capa)\n",
    "        capa_activacion = [X]\n",
    "        for i in range(0, len(self.capas) - 1):\n",
    "            x = np.dot(capa_activacion[i], self.pesos[i]) + self.bias[i]\n",
    "            y = self.activacion(x, self.activaciones[i])\n",
    "            capa_activacion.append(y)\n",
    "        return capa_activacion\n",
    "\n",
    "    def backpropagation(self, X, y, capa_activacion):\n",
    "        # delta de la capa de salida\n",
    "        if self.activaciones[-1] == \"softmax\":\n",
    "            delta_out = capa_activacion[-1] - y\n",
    "        else:\n",
    "            error = capa_activacion[-1] - y\n",
    "            delta_out = error * self.activacion_derivada(capa_activacion[-1], self.activaciones[-1])\n",
    "\n",
    "        # Lista de deltas alineada con self.pesos: deltas[i] corresponde al delta de a[i+1]\n",
    "        n_pesos = len(self.pesos)\n",
    "        deltas = [None] * n_pesos\n",
    "        deltas[-1] = delta_out\n",
    "\n",
    "        # Propagar hacia atrás los deltas para capas ocultas\n",
    "        # para i = n_pesos-2 .. 0\n",
    "        for i in range(n_pesos - 2, -1, -1):\n",
    "            # deltas[i+1] tiene forma (n_{i+2},) => np.dot(..., pesos[i+1].T) -> (n_{i+1},)\n",
    "            deltas[i] = np.dot(deltas[i+1], self.pesos[i+1].T) * self.activacion_derivada(\n",
    "                capa_activacion[i+1], self.activaciones[i]\n",
    "            )\n",
    "\n",
    "        # Actualizar pesos y bias\n",
    "        for i in range(n_pesos):\n",
    "            a = capa_activacion[i].reshape(-1, 1)        # (n_i,1)\n",
    "            d = deltas[i].reshape(1, -1)                # (1, n_{i+1})\n",
    "            self.pesos[i] -= self.alpha * np.dot(a, d)  # (n_i, n_{i+1})\n",
    "            self.bias[i] -= self.alpha * deltas[i]\n",
    "\n",
    "    def entrenar(self, X, y, epochs):\n",
    "        inicio = time.time()\n",
    "        for epoch in range(epochs):\n",
    "            # Entrenamiento por cada muestra\n",
    "            for i in range(len(X)):\n",
    "                capa_activacion = self.feedforward(X[i])\n",
    "                self.backpropagation(X[i], y[i], capa_activacion)\n",
    "\n",
    "            # Cada 10 épocas, imprimir progreso\n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                # Calcular loss y accuracy sobre el conjunto de entrenamiento\n",
    "                losses = []\n",
    "                aciertos = 0\n",
    "                for i in range(len(X)):\n",
    "                    pred = self.predecir(X[i])\n",
    "                    if self.activaciones[-1] == \"softmax\":\n",
    "                        # Cross-entropy loss\n",
    "                        # Agregamos pequeña constante para estabilidad numérica\n",
    "                        loss = -np.sum(y[i] * np.log(pred + 1e-9))\n",
    "                    else:\n",
    "                        # MSE loss\n",
    "                        loss = np.mean((pred - y[i]) ** 2)\n",
    "                    losses.append(loss)\n",
    "                    if np.argmax(pred) == np.argmax(y[i]):\n",
    "                        aciertos += 1\n",
    "                loss_prom = np.mean(losses)\n",
    "                acc = aciertos / len(X)\n",
    "\n",
    "                print(f\"Epoch {epoch+1}/{epochs} completado | Loss: {loss_prom:.4f} | Accuracy: {acc*100:.2f}%\")\n",
    "        \n",
    "        fin = time.time()\n",
    "        return fin - inicio\n",
    "\n",
    "    def predecir(self, X):\n",
    "        capa_activacion = self.feedforward(X)\n",
    "        return capa_activacion[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargar MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = fetch_openml('mnist_784', version=1)\n",
    "X = mnist.data.astype(np.float32)\n",
    "y = mnist.target.astype(int)\n",
    "\n",
    "# Normalizar a rango [0,1]\n",
    "X /= 255.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estandarizar los datos (media 0, varianza 1)\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# One-hot encoding de las etiquetas\n",
    "Y = np.eye(10)[y]\n",
    "\n",
    "# Dividir en entrenamiento y prueba\n",
    "X_entrenamiento, X_prueba, y_entrenamiento, y_prueba = train_test_split(\n",
    "    X, Y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crear y entrenar la red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/50 completado | Loss: 0.0095 | Accuracy: 99.86%\n",
      "Epoch 20/50 completado | Loss: 0.0016 | Accuracy: 100.00%\n",
      "Epoch 30/50 completado | Loss: 0.0008 | Accuracy: 100.00%\n",
      "Epoch 40/50 completado | Loss: 0.0005 | Accuracy: 100.00%\n",
      "Epoch 50/50 completado | Loss: 0.0004 | Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "perceptron = PerceptronMulticapa(capas=[784, 128, 64, 10], activaciones=[\"sigmoid\",\"sigmoid\", \"softmax\"], alpha=0.01)\n",
    "tiempo_entrenamiento = perceptron.entrenar(X_entrenamiento, y_entrenamiento, epochs=50)\n",
    "\n",
    "# Evaluar con un subconjunto (por tiempo)\n",
    "predicciones = []\n",
    "y_reales = []\n",
    "\n",
    "for i in range(500):\n",
    "    salida = perceptron.predecir(X_prueba[i])\n",
    "    pred_clase = np.argmax(salida)\n",
    "    real_clase = np.argmax(y_prueba[i])\n",
    "    predicciones.append(pred_clase)\n",
    "    y_reales.append(real_clase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Resultados del modelo:\n",
      "Accuracy:  0.9640\n",
      "Precision: 0.9625\n",
      "Recall:    0.9643\n",
      "F1-score:  0.9630\n",
      "Tiempo de entrenamiento: 646.76 segundos\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_reales, predicciones)\n",
    "precision = precision_score(y_reales, predicciones, average='macro')\n",
    "recall = recall_score(y_reales, predicciones, average='macro')\n",
    "f1 = f1_score(y_reales, predicciones, average='macro')\n",
    "\n",
    "print(\"\\nResultados del modelo:\")\n",
    "print(f\"Accuracy:  {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall:    {recall:.4f}\")\n",
    "print(f\"F1-score:  {f1:.4f}\")\n",
    "print(f\"Tiempo de entrenamiento: {tiempo_entrenamiento:.2f} segundos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createMLPClassifier(X, y, x_test, y_test, capas, activaciones, alpha=0.1, epochs=5):\n",
    "    \n",
    "    assert len(capas) == len(activaciones) + 1, \"Número de activaciones debe coincidir con número de capas\"\n",
    "\n",
    "    n_clases = len(np.unique(y))\n",
    "    perceptron = PerceptronMulticapa(capas=capas, activaciones=activaciones, alpha=0.01)\n",
    "\n",
    "    start_time = time.perf_counter()\n",
    "    perceptron.entrenar(X, np.eye(n_clases)[y], epochs=epochs)\n",
    "    end_time = time.perf_counter()\n",
    "    \n",
    "    # Hacer predicciones sobre el conjunto de prueba\n",
    "    predicciones = []\n",
    "    for i in range(len(x_test)):\n",
    "        prediccion = perceptron.predecir(x_test[i])\n",
    "        prediccion_clase = np.argmax(prediccion)\n",
    "        predicciones.append(prediccion_clase)\n",
    "    \n",
    "    # Calcular la precisión de las predicciones\n",
    "    precision = precision_score(y_test, predicciones, average='macro')\n",
    "    recall = recall_score(y_test , predicciones, average='macro')\n",
    "    f1 = f1_score(y_test, predicciones, average='macro')\n",
    "    accuracy = accuracy_score(y_test, predicciones)\n",
    "    print(f\"Precisión: {precision}\")\n",
    "    print(f\"Recall: {recall}\")\n",
    "    print(f\"F1: {f1}\")\n",
    "    print(f\"Exactitud: {accuracy}\")\n",
    "    print(f\"Tiempo de entrenamiento: {end_time - start_time}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP con Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.InputLayer(shape=[784]))\n",
    "model.add(tf.keras.layers.Dense(20, activation=\"sigmoid\"))\n",
    "model.add(tf.keras.layers.Dense(61, activation=\"sigmoid\"))\n",
    "model.add(tf.keras.layers.Dense(10, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\", optimizer=tf.keras.optimizers.SGD(learning_rate=0.01), metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "700/700 - 1s - 2ms/step - accuracy: 0.2872 - loss: 2.2474 - val_accuracy: 0.3669 - val_loss: 2.1629\n",
      "Epoch 2/50\n",
      "700/700 - 1s - 1ms/step - accuracy: 0.4809 - loss: 2.0677 - val_accuracy: 0.4994 - val_loss: 1.9638\n",
      "Epoch 3/50\n",
      "700/700 - 1s - 1ms/step - accuracy: 0.5503 - loss: 1.8375 - val_accuracy: 0.5710 - val_loss: 1.7128\n",
      "Epoch 4/50\n",
      "700/700 - 1s - 1ms/step - accuracy: 0.6181 - loss: 1.5837 - val_accuracy: 0.6439 - val_loss: 1.4664\n",
      "Epoch 5/50\n",
      "700/700 - 1s - 1ms/step - accuracy: 0.6873 - loss: 1.3543 - val_accuracy: 0.7132 - val_loss: 1.2574\n",
      "Epoch 6/50\n",
      "700/700 - 1s - 1ms/step - accuracy: 0.7405 - loss: 1.1646 - val_accuracy: 0.7572 - val_loss: 1.0877\n",
      "Epoch 7/50\n",
      "700/700 - 1s - 2ms/step - accuracy: 0.7772 - loss: 1.0113 - val_accuracy: 0.7878 - val_loss: 0.9508\n",
      "Epoch 8/50\n",
      "700/700 - 1s - 1ms/step - accuracy: 0.8066 - loss: 0.8875 - val_accuracy: 0.8124 - val_loss: 0.8395\n",
      "Epoch 9/50\n",
      "700/700 - 1s - 2ms/step - accuracy: 0.8301 - loss: 0.7862 - val_accuracy: 0.8338 - val_loss: 0.7479\n",
      "Epoch 10/50\n",
      "700/700 - 1s - 2ms/step - accuracy: 0.8485 - loss: 0.7026 - val_accuracy: 0.8510 - val_loss: 0.6721\n",
      "Epoch 11/50\n",
      "700/700 - 1s - 1ms/step - accuracy: 0.8615 - loss: 0.6336 - val_accuracy: 0.8636 - val_loss: 0.6098\n",
      "Epoch 12/50\n",
      "700/700 - 1s - 1ms/step - accuracy: 0.8710 - loss: 0.5771 - val_accuracy: 0.8728 - val_loss: 0.5592\n",
      "Epoch 13/50\n",
      "700/700 - 1s - 1ms/step - accuracy: 0.8796 - loss: 0.5311 - val_accuracy: 0.8799 - val_loss: 0.5183\n",
      "Epoch 14/50\n",
      "700/700 - 1s - 1ms/step - accuracy: 0.8860 - loss: 0.4938 - val_accuracy: 0.8846 - val_loss: 0.4854\n",
      "Epoch 15/50\n",
      "700/700 - 1s - 1ms/step - accuracy: 0.8905 - loss: 0.4634 - val_accuracy: 0.8884 - val_loss: 0.4586\n",
      "Epoch 16/50\n",
      "700/700 - 1s - 1ms/step - accuracy: 0.8941 - loss: 0.4385 - val_accuracy: 0.8911 - val_loss: 0.4366\n",
      "Epoch 17/50\n",
      "700/700 - 1s - 1ms/step - accuracy: 0.8972 - loss: 0.4177 - val_accuracy: 0.8938 - val_loss: 0.4184\n",
      "Epoch 18/50\n",
      "700/700 - 1s - 1ms/step - accuracy: 0.8999 - loss: 0.4002 - val_accuracy: 0.8956 - val_loss: 0.4031\n",
      "Epoch 19/50\n",
      "700/700 - 1s - 1ms/step - accuracy: 0.9027 - loss: 0.3852 - val_accuracy: 0.8971 - val_loss: 0.3901\n",
      "Epoch 20/50\n",
      "700/700 - 1s - 1ms/step - accuracy: 0.9049 - loss: 0.3722 - val_accuracy: 0.8988 - val_loss: 0.3788\n",
      "Epoch 21/50\n",
      "700/700 - 1s - 1ms/step - accuracy: 0.9071 - loss: 0.3608 - val_accuracy: 0.9005 - val_loss: 0.3689\n",
      "Epoch 22/50\n",
      "700/700 - 1s - 1ms/step - accuracy: 0.9087 - loss: 0.3507 - val_accuracy: 0.9021 - val_loss: 0.3602\n",
      "Epoch 23/50\n",
      "700/700 - 1s - 2ms/step - accuracy: 0.9105 - loss: 0.3416 - val_accuracy: 0.9038 - val_loss: 0.3524\n",
      "Epoch 24/50\n",
      "700/700 - 1s - 2ms/step - accuracy: 0.9118 - loss: 0.3333 - val_accuracy: 0.9049 - val_loss: 0.3454\n",
      "Epoch 25/50\n",
      "700/700 - 1s - 1ms/step - accuracy: 0.9133 - loss: 0.3258 - val_accuracy: 0.9072 - val_loss: 0.3391\n",
      "Epoch 26/50\n",
      "700/700 - 1s - 1ms/step - accuracy: 0.9144 - loss: 0.3188 - val_accuracy: 0.9081 - val_loss: 0.3333\n",
      "Epoch 27/50\n",
      "700/700 - 1s - 1ms/step - accuracy: 0.9155 - loss: 0.3124 - val_accuracy: 0.9102 - val_loss: 0.3281\n",
      "Epoch 28/50\n",
      "700/700 - 1s - 1ms/step - accuracy: 0.9165 - loss: 0.3064 - val_accuracy: 0.9112 - val_loss: 0.3232\n",
      "Epoch 29/50\n",
      "700/700 - 1s - 1ms/step - accuracy: 0.9181 - loss: 0.3008 - val_accuracy: 0.9121 - val_loss: 0.3186\n",
      "Epoch 30/50\n",
      "700/700 - 1s - 1ms/step - accuracy: 0.9193 - loss: 0.2956 - val_accuracy: 0.9133 - val_loss: 0.3144\n",
      "Epoch 31/50\n",
      "700/700 - 1s - 1ms/step - accuracy: 0.9210 - loss: 0.2907 - val_accuracy: 0.9137 - val_loss: 0.3105\n",
      "Epoch 32/50\n",
      "700/700 - 1s - 1ms/step - accuracy: 0.9223 - loss: 0.2860 - val_accuracy: 0.9148 - val_loss: 0.3068\n",
      "Epoch 33/50\n",
      "700/700 - 1s - 1ms/step - accuracy: 0.9231 - loss: 0.2816 - val_accuracy: 0.9153 - val_loss: 0.3033\n",
      "Epoch 34/50\n",
      "700/700 - 1s - 1ms/step - accuracy: 0.9244 - loss: 0.2774 - val_accuracy: 0.9161 - val_loss: 0.3000\n",
      "Epoch 35/50\n",
      "700/700 - 1s - 1ms/step - accuracy: 0.9254 - loss: 0.2734 - val_accuracy: 0.9167 - val_loss: 0.2968\n",
      "Epoch 36/50\n",
      "700/700 - 1s - 1ms/step - accuracy: 0.9262 - loss: 0.2696 - val_accuracy: 0.9175 - val_loss: 0.2939\n",
      "Epoch 37/50\n",
      "700/700 - 1s - 1ms/step - accuracy: 0.9270 - loss: 0.2660 - val_accuracy: 0.9184 - val_loss: 0.2911\n",
      "Epoch 38/50\n",
      "700/700 - 1s - 1ms/step - accuracy: 0.9283 - loss: 0.2625 - val_accuracy: 0.9189 - val_loss: 0.2884\n",
      "Epoch 39/50\n",
      "700/700 - 1s - 1ms/step - accuracy: 0.9289 - loss: 0.2592 - val_accuracy: 0.9197 - val_loss: 0.2858\n",
      "Epoch 40/50\n",
      "700/700 - 1s - 1ms/step - accuracy: 0.9297 - loss: 0.2559 - val_accuracy: 0.9204 - val_loss: 0.2834\n",
      "Epoch 41/50\n",
      "700/700 - 1s - 1ms/step - accuracy: 0.9305 - loss: 0.2529 - val_accuracy: 0.9209 - val_loss: 0.2810\n",
      "Epoch 42/50\n",
      "700/700 - 1s - 1ms/step - accuracy: 0.9312 - loss: 0.2499 - val_accuracy: 0.9211 - val_loss: 0.2788\n",
      "Epoch 43/50\n",
      "700/700 - 1s - 1ms/step - accuracy: 0.9320 - loss: 0.2470 - val_accuracy: 0.9215 - val_loss: 0.2767\n",
      "Epoch 44/50\n",
      "700/700 - 1s - 1ms/step - accuracy: 0.9327 - loss: 0.2443 - val_accuracy: 0.9218 - val_loss: 0.2747\n",
      "Epoch 45/50\n",
      "700/700 - 1s - 1ms/step - accuracy: 0.9335 - loss: 0.2416 - val_accuracy: 0.9223 - val_loss: 0.2727\n",
      "Epoch 46/50\n",
      "700/700 - 1s - 1ms/step - accuracy: 0.9342 - loss: 0.2391 - val_accuracy: 0.9231 - val_loss: 0.2708\n",
      "Epoch 47/50\n",
      "700/700 - 1s - 1ms/step - accuracy: 0.9349 - loss: 0.2366 - val_accuracy: 0.9236 - val_loss: 0.2690\n",
      "Epoch 48/50\n",
      "700/700 - 1s - 1ms/step - accuracy: 0.9352 - loss: 0.2342 - val_accuracy: 0.9239 - val_loss: 0.2673\n",
      "Epoch 49/50\n",
      "700/700 - 1s - 1ms/step - accuracy: 0.9361 - loss: 0.2318 - val_accuracy: 0.9246 - val_loss: 0.2656\n",
      "Epoch 50/50\n",
      "700/700 - 1s - 1ms/step - accuracy: 0.9368 - loss: 0.2296 - val_accuracy: 0.9249 - val_loss: 0.2640\n"
     ]
    }
   ],
   "source": [
    "start_time = time.perf_counter()\n",
    "history = model.fit(\n",
    "    X_entrenamiento,\n",
    "    y_entrenamiento,\n",
    "    epochs=50,\n",
    "    batch_size=64,\n",
    "    verbose=2,\n",
    "    validation_split=0.2\n",
    ")\n",
    "end_time = time.perf_counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_predictions = model.predict(X_prueba, verbose=0)\n",
    "model_predictions = model_predictions.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisión: 0.9195\n",
      "Recall: 0.9194\n",
      "F1-score: 0.9193\n",
      "Exactitud: 0.9204\n",
      "Tiempo de entrenamiento: 48.18 segundos\n"
     ]
    }
   ],
   "source": [
    "y_prueba_labels = y_prueba.argmax(axis=1)\n",
    "\n",
    "precision = precision_score(y_prueba_labels, model_predictions, average='macro')\n",
    "recall = recall_score(y_prueba_labels, model_predictions, average='macro')\n",
    "f1 = f1_score(y_prueba_labels, model_predictions, average='macro')\n",
    "accuracy = accuracy_score(y_prueba_labels, model_predictions)\n",
    "\n",
    "print(f\"Precisión: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-score: {f1:.4f}\")\n",
    "print(f\"Exactitud: {accuracy:.4f}\")\n",
    "print(f\"Tiempo de entrenamiento: {end_time - start_time:.2f} segundos\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Patrones",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
